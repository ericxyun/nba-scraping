{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "dates = ['00-01', '01-02', '02-03', '03-04', '04-05', '05-06', '06-07', '07-08', '08-09', '09-10', '10-11', '11-12', '12-13', '13-14', '14-15', '15-16', '16-17', '17-18']\n",
    "\n",
    "for i, item in enumerate(dates):\n",
    "    data_list[i].append(pd.read_csv(\"data/historic/avg/\" + str(item) + \"_avg.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_list[0][0].append([data_list[x][0] for x in range(1, 18)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>PLAYER</th>\n",
       "      <th>TEAM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>GP</th>\n",
       "      <th>WLP</th>\n",
       "      <th>W</th>\n",
       "      <th>L</th>\n",
       "      <th>MIN</th>\n",
       "      <th>PTS</th>\n",
       "      <th>FGM</th>\n",
       "      <th>FGA</th>\n",
       "      <th>FG%</th>\n",
       "      <th>3PM</th>\n",
       "      <th>3PA</th>\n",
       "      <th>3P%</th>\n",
       "      <th>FTM</th>\n",
       "      <th>FTA</th>\n",
       "      <th>FT%</th>\n",
       "      <th>OREB</th>\n",
       "      <th>DREB</th>\n",
       "      <th>REB</th>\n",
       "      <th>AST</th>\n",
       "      <th>TOV</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>PF</th>\n",
       "      <th>FP</th>\n",
       "      <th>DD2</th>\n",
       "      <th>TD3</th>\n",
       "      <th>+/-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00-01</td>\n",
       "      <td>Allen Iverson</td>\n",
       "      <td>PHI</td>\n",
       "      <td>26.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>50.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>41.9</td>\n",
       "      <td>31.1</td>\n",
       "      <td>10.7</td>\n",
       "      <td>25.5</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>4.3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>10.1</td>\n",
       "      <td>81.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>47.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00-01</td>\n",
       "      <td>Jerry Stackhouse</td>\n",
       "      <td>DET</td>\n",
       "      <td>26.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.387500</td>\n",
       "      <td>31.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>40.1</td>\n",
       "      <td>29.8</td>\n",
       "      <td>9.7</td>\n",
       "      <td>24.1</td>\n",
       "      <td>40.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>5.9</td>\n",
       "      <td>35.1</td>\n",
       "      <td>8.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00-01</td>\n",
       "      <td>Shaquille O'Neal</td>\n",
       "      <td>LAL</td>\n",
       "      <td>29.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.689189</td>\n",
       "      <td>51.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>28.7</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>57.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>13.1</td>\n",
       "      <td>51.3</td>\n",
       "      <td>3.9</td>\n",
       "      <td>8.8</td>\n",
       "      <td>12.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.5</td>\n",
       "      <td>56.8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00-01</td>\n",
       "      <td>Kobe Bryant</td>\n",
       "      <td>LAL</td>\n",
       "      <td>22.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.661765</td>\n",
       "      <td>45.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>10.3</td>\n",
       "      <td>22.2</td>\n",
       "      <td>46.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>30.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>85.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3.3</td>\n",
       "      <td>46.7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00-01</td>\n",
       "      <td>Vince Carter</td>\n",
       "      <td>TOR</td>\n",
       "      <td>24.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>45.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>39.7</td>\n",
       "      <td>27.6</td>\n",
       "      <td>10.2</td>\n",
       "      <td>22.1</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>40.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>76.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>45.7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DATE            PLAYER TEAM   AGE    GP       WLP     W     L   MIN   PTS  \\\n",
       "0  00-01     Allen Iverson  PHI  26.0  71.0  0.704225  50.0  21.0  41.9  31.1   \n",
       "1  00-01  Jerry Stackhouse  DET  26.0  80.0  0.387500  31.0  49.0  40.1  29.8   \n",
       "2  00-01  Shaquille O'Neal  LAL  29.0  74.0  0.689189  51.0  23.0  39.5  28.7   \n",
       "3  00-01       Kobe Bryant  LAL  22.0  68.0  0.661765  45.0  23.0  41.0  28.5   \n",
       "4  00-01      Vince Carter  TOR  24.0  75.0  0.600000  45.0  30.0  39.7  27.6   \n",
       "\n",
       "    FGM   FGA   FG%  3PM  3PA   3P%  FTM   FTA   FT%  OREB  DREB   REB  AST  \\\n",
       "0  10.7  25.5  42.0  1.4  4.3  32.0  8.2  10.1  81.4   0.7   3.1   3.8  4.6   \n",
       "1   9.7  24.1  40.2  2.1  5.9  35.1  8.3  10.1  82.2   1.2   2.7   3.9  5.1   \n",
       "2  11.0  19.2  57.2  0.0  0.0   0.0  6.7  13.1  51.3   3.9   8.8  12.7  3.7   \n",
       "3  10.3  22.2  46.4  0.9  2.9  30.5  7.0   8.2  85.3   1.5   4.3   5.9  5.0   \n",
       "4  10.2  22.1  46.0  2.2  5.3  40.8  5.1   6.7  76.5   2.3   3.2   5.5  3.9   \n",
       "\n",
       "   TOV  STL  BLK   PF    FP   DD2  TD3  +/-  \n",
       "0  3.3  2.5  0.3  2.1  47.6   4.0  0.0  5.2  \n",
       "1  4.1  1.2  0.7  2.0  43.8   5.0  1.0 -0.8  \n",
       "2  2.9  0.6  2.8  3.5  56.8  60.0  0.0  6.3  \n",
       "3  3.2  1.7  0.6  3.3  46.7  10.0  2.0  5.4  \n",
       "4  2.2  1.5  1.1  2.7  45.7  10.0  0.0  4.3  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.WLP < 1, 'WLP'] = data.loc[data.WLP < 1, 'WLP'] *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_winners = {\n",
    "        '00-01':'LAL',\n",
    "        '01-02':'LAL',\n",
    "        '02-03':'SAS',\n",
    "        '03-04':'DET',\n",
    "        '04-05':'SAS',\n",
    "        '05-06':'MIA',\n",
    "        '06-07':'SAS',\n",
    "        '07-08':'BOS',\n",
    "        '08-09':'LAL',\n",
    "        '09-10':'LAL',\n",
    "        '10-11':'DAL',\n",
    "        '11-12':'MIA',\n",
    "        '12-13':'MIA',\n",
    "        '13-14':'SAS',\n",
    "        '14-15':'GSW',\n",
    "        '15-16':'CLE',\n",
    "        '16-17':'GSW',\n",
    "        '17-18':'GSW'\n",
    "    }\n",
    "\n",
    "data['WT'] = ''\n",
    "\n",
    "for i, j in game_winners.items():\n",
    "    data.loc[(data.DATE == i) & (data['TEAM'] == j), 'WT'] = 1\n",
    "    data.loc[(data.DATE == i) & (data['TEAM'] != j), 'WT'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What I want to figure out:__ I want to change the values of a column based on the values of two other columns.  \n",
    "__Question:__ In one step, how can I filter the DataFrame based on the values of two of their columns in order to change the values of a third column?\n",
    "__Solution:__  \n",
    "`df.loc[(df.<column1> <comparison operators> '<value>') <bitwise operators> (df.<condition2> <comparison operators> '<value>'), '<column you want to change>'] = '<the value you want to change to>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['WT'] = ''\n",
    "for i, j in game_winners.items():\n",
    "    data.loc[(data.DATE == i) & (data['TEAM'] == j), 'WT'] = 1\n",
    "    data.loc[(data.DATE == i) & (data['TEAM'] != j), 'WT'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>PLAYER</th>\n",
       "      <th>TEAM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>GP</th>\n",
       "      <th>WLP</th>\n",
       "      <th>W</th>\n",
       "      <th>L</th>\n",
       "      <th>MIN</th>\n",
       "      <th>PTS</th>\n",
       "      <th>FGM</th>\n",
       "      <th>FGA</th>\n",
       "      <th>FG%</th>\n",
       "      <th>3PM</th>\n",
       "      <th>3PA</th>\n",
       "      <th>3P%</th>\n",
       "      <th>FTM</th>\n",
       "      <th>FTA</th>\n",
       "      <th>FT%</th>\n",
       "      <th>OREB</th>\n",
       "      <th>DREB</th>\n",
       "      <th>REB</th>\n",
       "      <th>AST</th>\n",
       "      <th>TOV</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>PF</th>\n",
       "      <th>FP</th>\n",
       "      <th>DD2</th>\n",
       "      <th>TD3</th>\n",
       "      <th>+/-</th>\n",
       "      <th>WT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00-01</td>\n",
       "      <td>Allen Iverson</td>\n",
       "      <td>PHI</td>\n",
       "      <td>26.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>70.422535</td>\n",
       "      <td>50.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>41.9</td>\n",
       "      <td>31.1</td>\n",
       "      <td>10.7</td>\n",
       "      <td>25.5</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>4.3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>10.1</td>\n",
       "      <td>81.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>47.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00-01</td>\n",
       "      <td>Jerry Stackhouse</td>\n",
       "      <td>DET</td>\n",
       "      <td>26.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>38.750000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>40.1</td>\n",
       "      <td>29.8</td>\n",
       "      <td>9.7</td>\n",
       "      <td>24.1</td>\n",
       "      <td>40.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>5.9</td>\n",
       "      <td>35.1</td>\n",
       "      <td>8.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00-01</td>\n",
       "      <td>Shaquille O'Neal</td>\n",
       "      <td>LAL</td>\n",
       "      <td>29.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>68.918919</td>\n",
       "      <td>51.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>28.7</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>57.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>13.1</td>\n",
       "      <td>51.3</td>\n",
       "      <td>3.9</td>\n",
       "      <td>8.8</td>\n",
       "      <td>12.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.5</td>\n",
       "      <td>56.8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00-01</td>\n",
       "      <td>Kobe Bryant</td>\n",
       "      <td>LAL</td>\n",
       "      <td>22.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>66.176471</td>\n",
       "      <td>45.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>10.3</td>\n",
       "      <td>22.2</td>\n",
       "      <td>46.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>30.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>85.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3.3</td>\n",
       "      <td>46.7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00-01</td>\n",
       "      <td>Vince Carter</td>\n",
       "      <td>TOR</td>\n",
       "      <td>24.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>45.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>39.7</td>\n",
       "      <td>27.6</td>\n",
       "      <td>10.2</td>\n",
       "      <td>22.1</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>40.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>76.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>45.7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DATE            PLAYER TEAM   AGE    GP        WLP     W     L   MIN  \\\n",
       "0  00-01     Allen Iverson  PHI  26.0  71.0  70.422535  50.0  21.0  41.9   \n",
       "1  00-01  Jerry Stackhouse  DET  26.0  80.0  38.750000  31.0  49.0  40.1   \n",
       "2  00-01  Shaquille O'Neal  LAL  29.0  74.0  68.918919  51.0  23.0  39.5   \n",
       "3  00-01       Kobe Bryant  LAL  22.0  68.0  66.176471  45.0  23.0  41.0   \n",
       "4  00-01      Vince Carter  TOR  24.0  75.0  60.000000  45.0  30.0  39.7   \n",
       "\n",
       "    PTS   FGM   FGA   FG%  3PM  3PA   3P%  FTM   FTA   FT%  OREB  DREB   REB  \\\n",
       "0  31.1  10.7  25.5  42.0  1.4  4.3  32.0  8.2  10.1  81.4   0.7   3.1   3.8   \n",
       "1  29.8   9.7  24.1  40.2  2.1  5.9  35.1  8.3  10.1  82.2   1.2   2.7   3.9   \n",
       "2  28.7  11.0  19.2  57.2  0.0  0.0   0.0  6.7  13.1  51.3   3.9   8.8  12.7   \n",
       "3  28.5  10.3  22.2  46.4  0.9  2.9  30.5  7.0   8.2  85.3   1.5   4.3   5.9   \n",
       "4  27.6  10.2  22.1  46.0  2.2  5.3  40.8  5.1   6.7  76.5   2.3   3.2   5.5   \n",
       "\n",
       "   AST  TOV  STL  BLK   PF    FP   DD2  TD3  +/-  WT  \n",
       "0  4.6  3.3  2.5  0.3  2.1  47.6   4.0  0.0  5.2   0  \n",
       "1  5.1  4.1  1.2  0.7  2.0  43.8   5.0  1.0 -0.8   0  \n",
       "2  3.7  2.9  0.6  2.8  3.5  56.8  60.0  0.0  6.3   1  \n",
       "3  5.0  3.2  1.7  0.6  3.3  46.7  10.0  2.0  5.4   1  \n",
       "4  3.9  2.2  1.5  1.1  2.7  45.7  10.0  0.0  4.3   0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Keras__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>GP</th>\n",
       "      <th>WLP</th>\n",
       "      <th>W</th>\n",
       "      <th>L</th>\n",
       "      <th>MIN</th>\n",
       "      <th>PTS</th>\n",
       "      <th>FGM</th>\n",
       "      <th>FGA</th>\n",
       "      <th>FG%</th>\n",
       "      <th>3PM</th>\n",
       "      <th>3PA</th>\n",
       "      <th>3P%</th>\n",
       "      <th>FTM</th>\n",
       "      <th>FTA</th>\n",
       "      <th>FT%</th>\n",
       "      <th>OREB</th>\n",
       "      <th>DREB</th>\n",
       "      <th>REB</th>\n",
       "      <th>AST</th>\n",
       "      <th>TOV</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>PF</th>\n",
       "      <th>FP</th>\n",
       "      <th>DD2</th>\n",
       "      <th>TD3</th>\n",
       "      <th>WT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>70.422535</td>\n",
       "      <td>50.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>41.9</td>\n",
       "      <td>31.1</td>\n",
       "      <td>10.7</td>\n",
       "      <td>25.5</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>4.3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>10.1</td>\n",
       "      <td>81.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>47.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AGE    GP        WLP     W     L   MIN   PTS   FGM   FGA   FG%  3PM  3PA  \\\n",
       "0  26.0  71.0  70.422535  50.0  21.0  41.9  31.1  10.7  25.5  42.0  1.4  4.3   \n",
       "\n",
       "    3P%  FTM   FTA   FT%  OREB  DREB  REB  AST  TOV  STL  BLK   PF    FP  DD2  \\\n",
       "0  32.0  8.2  10.1  81.4   0.7   3.1  3.8  4.6  3.3  2.5  0.3  2.1  47.6  4.0   \n",
       "\n",
       "   TD3  WT  \n",
       "0  0.0   0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.drop(['TEAM', 'DATE', 'PLAYER', '+/-'], axis=1)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df.iloc[:, 0:-1], drop_first=True).values\n",
    "y = df.iloc[:, -1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7800, 27)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size = 0,2, \n",
    "                                                    random_state = 0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7800/7800 [==============================] - 1s 186us/step - loss: 0.2472 - acc: 0.9672\n",
      "Epoch 2/100\n",
      "7800/7800 [==============================] - 1s 141us/step - loss: 0.1452 - acc: 0.9674\n",
      "Epoch 3/100\n",
      "7800/7800 [==============================] - 1s 141us/step - loss: 0.1441 - acc: 0.9674\n",
      "Epoch 4/100\n",
      "7800/7800 [==============================] - 1s 141us/step - loss: 0.1443 - acc: 0.9674\n",
      "Epoch 5/100\n",
      "7800/7800 [==============================] - 1s 141us/step - loss: 0.1440 - acc: 0.9674\n",
      "Epoch 6/100\n",
      "7800/7800 [==============================] - 1s 144us/step - loss: 0.1457 - acc: 0.9674\n",
      "Epoch 7/100\n",
      "7800/7800 [==============================] - 1s 176us/step - loss: 0.1440 - acc: 0.9674\n",
      "Epoch 8/100\n",
      "7800/7800 [==============================] - 2s 197us/step - loss: 0.1453 - acc: 0.9674\n",
      "Epoch 9/100\n",
      "7800/7800 [==============================] - 1s 145us/step - loss: 0.1438 - acc: 0.9674\n",
      "Epoch 10/100\n",
      "7800/7800 [==============================] - 1s 151us/step - loss: 0.1449 - acc: 0.9674\n",
      "Epoch 11/100\n",
      "7800/7800 [==============================] - 1s 156us/step - loss: 0.1449 - acc: 0.9674\n",
      "Epoch 12/100\n",
      "7800/7800 [==============================] - 1s 149us/step - loss: 0.1444 - acc: 0.9674\n",
      "Epoch 13/100\n",
      "7800/7800 [==============================] - 1s 147us/step - loss: 0.1454 - acc: 0.9674\n",
      "Epoch 14/100\n",
      "7800/7800 [==============================] - 1s 155us/step - loss: 0.1453 - acc: 0.9674\n",
      "Epoch 15/100\n",
      "7800/7800 [==============================] - 1s 149us/step - loss: 0.1456 - acc: 0.9674\n",
      "Epoch 16/100\n",
      "7800/7800 [==============================] - 1s 150us/step - loss: 0.1450 - acc: 0.9674\n",
      "Epoch 17/100\n",
      "7800/7800 [==============================] - 1s 142us/step - loss: 0.1460 - acc: 0.9674\n",
      "Epoch 18/100\n",
      "7800/7800 [==============================] - 1s 153us/step - loss: 0.1450 - acc: 0.9674\n",
      "Epoch 19/100\n",
      "7800/7800 [==============================] - 1s 148us/step - loss: 0.1438 - acc: 0.9674\n",
      "Epoch 20/100\n",
      "7800/7800 [==============================] - 1s 172us/step - loss: 0.1450 - acc: 0.9674\n",
      "Epoch 21/100\n",
      "7800/7800 [==============================] - 1s 173us/step - loss: 0.1446 - acc: 0.9674\n",
      "Epoch 22/100\n",
      "7800/7800 [==============================] - 2s 198us/step - loss: 0.1456 - acc: 0.9674\n",
      "Epoch 23/100\n",
      "7800/7800 [==============================] - 1s 158us/step - loss: 0.1444 - acc: 0.9674\n",
      "Epoch 24/100\n",
      "7800/7800 [==============================] - 1s 162us/step - loss: 0.1456 - acc: 0.9674\n",
      "Epoch 25/100\n",
      "7800/7800 [==============================] - 1s 150us/step - loss: 0.1449 - acc: 0.9674\n",
      "Epoch 26/100\n",
      "7800/7800 [==============================] - 1s 151us/step - loss: 0.1446 - acc: 0.9674\n",
      "Epoch 27/100\n",
      "7800/7800 [==============================] - 1s 149us/step - loss: 0.1452 - acc: 0.9674\n",
      "Epoch 28/100\n",
      "7800/7800 [==============================] - 1s 148us/step - loss: 0.1451 - acc: 0.9674\n",
      "Epoch 29/100\n",
      "7800/7800 [==============================] - 1s 149us/step - loss: 0.1457 - acc: 0.9674\n",
      "Epoch 30/100\n",
      "7800/7800 [==============================] - 1s 156us/step - loss: 0.1454 - acc: 0.9674\n",
      "Epoch 31/100\n",
      "7800/7800 [==============================] - 1s 146us/step - loss: 0.1450 - acc: 0.9674\n",
      "Epoch 32/100\n",
      "7800/7800 [==============================] - 1s 150us/step - loss: 0.1444 - acc: 0.9674\n",
      "Epoch 33/100\n",
      "7800/7800 [==============================] - 1s 161us/step - loss: 0.1453 - acc: 0.9674\n",
      "Epoch 34/100\n",
      "7800/7800 [==============================] - 1s 181us/step - loss: 0.1435 - acc: 0.9674\n",
      "Epoch 35/100\n",
      "7800/7800 [==============================] - 1s 149us/step - loss: 0.1453 - acc: 0.9674\n",
      "Epoch 36/100\n",
      "7800/7800 [==============================] - 1s 151us/step - loss: 0.1440 - acc: 0.9674\n",
      "Epoch 37/100\n",
      "7800/7800 [==============================] - 1s 151us/step - loss: 0.1444 - acc: 0.9674\n",
      "Epoch 38/100\n",
      "7800/7800 [==============================] - 1s 168us/step - loss: 0.1443 - acc: 0.9674\n",
      "Epoch 39/100\n",
      "7800/7800 [==============================] - 1s 160us/step - loss: 0.1447 - acc: 0.9674\n",
      "Epoch 40/100\n",
      "7800/7800 [==============================] - 1s 189us/step - loss: 0.1439 - acc: 0.9674\n",
      "Epoch 41/100\n",
      "7800/7800 [==============================] - 1s 182us/step - loss: 0.1452 - acc: 0.9674\n",
      "Epoch 42/100\n",
      "7800/7800 [==============================] - 1s 150us/step - loss: 0.1440 - acc: 0.9674\n",
      "Epoch 43/100\n",
      "7800/7800 [==============================] - 1s 154us/step - loss: 0.1450 - acc: 0.9674\n",
      "Epoch 44/100\n",
      "7800/7800 [==============================] - 1s 149us/step - loss: 0.1444 - acc: 0.9674\n",
      "Epoch 45/100\n",
      "7800/7800 [==============================] - 1s 142us/step - loss: 0.1445 - acc: 0.9674\n",
      "Epoch 46/100\n",
      "7800/7800 [==============================] - 1s 147us/step - loss: 0.1448 - acc: 0.9674\n",
      "Epoch 47/100\n",
      "7800/7800 [==============================] - 1s 183us/step - loss: 0.1453 - acc: 0.9674\n",
      "Epoch 48/100\n",
      "7800/7800 [==============================] - 1s 153us/step - loss: 0.1450 - acc: 0.9674\n",
      "Epoch 49/100\n",
      "7800/7800 [==============================] - 1s 145us/step - loss: 0.1450 - acc: 0.9674\n",
      "Epoch 50/100\n",
      "7800/7800 [==============================] - 1s 149us/step - loss: 0.1450 - acc: 0.9674\n",
      "Epoch 51/100\n",
      "7800/7800 [==============================] - 1s 153us/step - loss: 0.1453 - acc: 0.9674\n",
      "Epoch 52/100\n",
      "7800/7800 [==============================] - 1s 157us/step - loss: 0.1440 - acc: 0.9674\n",
      "Epoch 53/100\n",
      "7800/7800 [==============================] - 1s 150us/step - loss: 0.1437 - acc: 0.9674\n",
      "Epoch 54/100\n",
      "7800/7800 [==============================] - 1s 149us/step - loss: 0.1447 - acc: 0.9674\n",
      "Epoch 55/100\n",
      "7800/7800 [==============================] - 1s 154us/step - loss: 0.1438 - acc: 0.9674\n",
      "Epoch 56/100\n",
      "7800/7800 [==============================] - 1s 157us/step - loss: 0.1441 - acc: 0.9674\n",
      "Epoch 57/100\n",
      "7800/7800 [==============================] - 1s 150us/step - loss: 0.1448 - acc: 0.9674\n",
      "Epoch 58/100\n",
      "7800/7800 [==============================] - 1s 151us/step - loss: 0.1449 - acc: 0.9674\n",
      "Epoch 59/100\n",
      "7800/7800 [==============================] - 1s 149us/step - loss: 0.1438 - acc: 0.9674\n",
      "Epoch 60/100\n",
      "7800/7800 [==============================] - 1s 151us/step - loss: 0.1454 - acc: 0.9674\n",
      "Epoch 61/100\n",
      "7800/7800 [==============================] - 1s 148us/step - loss: 0.1443 - acc: 0.9674\n",
      "Epoch 62/100\n",
      "7800/7800 [==============================] - 1s 149us/step - loss: 0.1445 - acc: 0.9674\n",
      "Epoch 63/100\n",
      "7800/7800 [==============================] - 1s 152us/step - loss: 0.1443 - acc: 0.9674\n",
      "Epoch 64/100\n",
      "7800/7800 [==============================] - 1s 172us/step - loss: 0.1442 - acc: 0.9674\n",
      "Epoch 65/100\n",
      "7800/7800 [==============================] - 1s 177us/step - loss: 0.1453 - acc: 0.9674\n",
      "Epoch 66/100\n",
      "7800/7800 [==============================] - 2s 207us/step - loss: 0.1450 - acc: 0.9674\n",
      "Epoch 67/100\n",
      "7800/7800 [==============================] - 1s 178us/step - loss: 0.1443 - acc: 0.9674\n",
      "Epoch 68/100\n",
      "7800/7800 [==============================] - 1s 147us/step - loss: 0.1447 - acc: 0.9674\n",
      "Epoch 69/100\n",
      "7800/7800 [==============================] - 1s 147us/step - loss: 0.1450 - acc: 0.9674\n",
      "Epoch 70/100\n",
      "7800/7800 [==============================] - 1s 154us/step - loss: 0.1445 - acc: 0.9674\n",
      "Epoch 71/100\n",
      "7800/7800 [==============================] - 1s 151us/step - loss: 0.1443 - acc: 0.9674\n",
      "Epoch 72/100\n",
      "7800/7800 [==============================] - 1s 149us/step - loss: 0.1454 - acc: 0.9674\n",
      "Epoch 73/100\n",
      "7800/7800 [==============================] - 1s 148us/step - loss: 0.1451 - acc: 0.9674\n",
      "Epoch 74/100\n",
      "7800/7800 [==============================] - 1s 147us/step - loss: 0.1445 - acc: 0.9674\n",
      "Epoch 75/100\n",
      "7800/7800 [==============================] - 1s 147us/step - loss: 0.1441 - acc: 0.9674\n",
      "Epoch 76/100\n",
      "7800/7800 [==============================] - 1s 144us/step - loss: 0.1445 - acc: 0.9674\n",
      "Epoch 77/100\n",
      "7800/7800 [==============================] - 1s 149us/step - loss: 0.1448 - acc: 0.9674\n",
      "Epoch 78/100\n",
      "7800/7800 [==============================] - 1s 153us/step - loss: 0.1440 - acc: 0.9674\n",
      "Epoch 79/100\n",
      "7800/7800 [==============================] - 1s 151us/step - loss: 0.1450 - acc: 0.9674\n",
      "Epoch 80/100\n",
      "7800/7800 [==============================] - 1s 152us/step - loss: 0.1448 - acc: 0.9674\n",
      "Epoch 81/100\n",
      "7800/7800 [==============================] - 1s 150us/step - loss: 0.1443 - acc: 0.9674\n",
      "Epoch 82/100\n",
      "7800/7800 [==============================] - 1s 153us/step - loss: 0.1449 - acc: 0.9674\n",
      "Epoch 83/100\n",
      "7800/7800 [==============================] - 1s 144us/step - loss: 0.1444 - acc: 0.9674\n",
      "Epoch 84/100\n",
      "7800/7800 [==============================] - 1s 149us/step - loss: 0.1447 - acc: 0.9674\n",
      "Epoch 85/100\n",
      "7800/7800 [==============================] - 1s 147us/step - loss: 0.1437 - acc: 0.9674\n",
      "Epoch 86/100\n",
      "7800/7800 [==============================] - 1s 144us/step - loss: 0.1444 - acc: 0.9674\n",
      "Epoch 87/100\n",
      "7800/7800 [==============================] - 1s 146us/step - loss: 0.1446 - acc: 0.9674\n",
      "Epoch 88/100\n",
      "7800/7800 [==============================] - 1s 143us/step - loss: 0.1449 - acc: 0.9674\n",
      "Epoch 89/100\n",
      "7800/7800 [==============================] - 1s 147us/step - loss: 0.1447 - acc: 0.9674\n",
      "Epoch 90/100\n",
      "7800/7800 [==============================] - 1s 150us/step - loss: 0.1451 - acc: 0.9674\n",
      "Epoch 91/100\n",
      "7800/7800 [==============================] - 1s 146us/step - loss: 0.1436 - acc: 0.9674\n",
      "Epoch 92/100\n",
      "7800/7800 [==============================] - 1s 143us/step - loss: 0.1441 - acc: 0.9674\n",
      "Epoch 93/100\n",
      "7800/7800 [==============================] - 1s 150us/step - loss: 0.1448 - acc: 0.9674\n",
      "Epoch 94/100\n",
      "7800/7800 [==============================] - 1s 153us/step - loss: 0.1442 - acc: 0.9674\n",
      "Epoch 95/100\n",
      "7800/7800 [==============================] - 1s 151us/step - loss: 0.1459 - acc: 0.9674\n",
      "Epoch 96/100\n",
      "7800/7800 [==============================] - 1s 143us/step - loss: 0.1445 - acc: 0.9674\n",
      "Epoch 97/100\n",
      "7800/7800 [==============================] - 1s 145us/step - loss: 0.1443 - acc: 0.9674\n",
      "Epoch 98/100\n",
      "7800/7800 [==============================] - 1s 143us/step - loss: 0.1450 - acc: 0.9674\n",
      "Epoch 99/100\n",
      "7800/7800 [==============================] - 1s 149us/step - loss: 0.1443 - acc: 0.9674\n",
      "Epoch 100/100\n",
      "7800/7800 [==============================] - 1s 152us/step - loss: 0.1438 - acc: 0.9674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f87ffa7ef0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer with Dropout\n",
    "classifier.add(Dense(units=16, # 6 hidden layer nodes (input + output)/2 (11+1)/2\n",
    "                     kernel_initializer='uniform',\n",
    "                     activation='sigmoid', # relu = rectifier function\n",
    "                     input_dim=27)) # 11 features from X\n",
    "classifier.add(Dropout(rate=0.1))\n",
    "\n",
    "# Second input layer\n",
    "classifier.add(Dense(units=16, # 6 hidden layer nodes\n",
    "                     kernel_initializer='uniform',\n",
    "                     activation='sigmoid'))\n",
    "classifier.add(Dropout(rate=0.1))\n",
    "\n",
    "# Third input layer\n",
    "classifier.add(Dense(units=16, # 6 hidden layer nodes\n",
    "                     kernel_initializer='uniform',\n",
    "                     activation='sigmoid'))\n",
    "classifier.add(Dropout(rate=0.1))\n",
    "\n",
    "# Output layer\n",
    "classifier.add(Dense(units=1, # 6 hidden layer nodes\n",
    "                     kernel_initializer='uniform',\n",
    "                     activation='sigmoid'))\n",
    "\n",
    "# Compiling the ANN (Apply Stochastic Gradient Descent on ANN)\n",
    "classifier.compile(optimizer='adam', # gradient descent algorithm (adam algorithm))\n",
    "                   loss='binary_crossentropy',# logarithmic loss algorithm \n",
    "                   metrics=['accuracy']) \n",
    "\n",
    "# Fitting the ANN to the training set\n",
    "classifier.fit(X,\n",
    "               y,\n",
    "               batch_size=20, #\n",
    "               epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5616/5616 [==============================] - 1s 181us/step - loss: 0.3794 - acc: 0.9671\n",
      "Epoch 2/100\n",
      "5616/5616 [==============================] - 1s 140us/step - loss: 0.1297 - acc: 0.9688\n",
      "Epoch 3/100\n",
      "5616/5616 [==============================] - 1s 139us/step - loss: 0.1055 - acc: 0.9688\n",
      "Epoch 4/100\n",
      "5616/5616 [==============================] - 1s 142us/step - loss: 0.0929 - acc: 0.9688\n",
      "Epoch 5/100\n",
      "5616/5616 [==============================] - 1s 144us/step - loss: 0.0875 - acc: 0.9688\n",
      "Epoch 6/100\n",
      "5616/5616 [==============================] - 1s 147us/step - loss: 0.0842 - acc: 0.9688\n",
      "Epoch 7/100\n",
      "5616/5616 [==============================] - 1s 145us/step - loss: 0.0817 - acc: 0.9688\n",
      "Epoch 8/100\n",
      "5616/5616 [==============================] - 1s 143us/step - loss: 0.0801 - acc: 0.9688\n",
      "Epoch 9/100\n",
      "5616/5616 [==============================] - 1s 147us/step - loss: 0.0786 - acc: 0.9688\n",
      "Epoch 10/100\n",
      "5616/5616 [==============================] - 1s 144us/step - loss: 0.0774 - acc: 0.9688\n",
      "Epoch 11/100\n",
      "5616/5616 [==============================] - 1s 139us/step - loss: 0.0759 - acc: 0.9688\n",
      "Epoch 12/100\n",
      "5616/5616 [==============================] - 1s 141us/step - loss: 0.0753 - acc: 0.9688\n",
      "Epoch 13/100\n",
      "5616/5616 [==============================] - 1s 142us/step - loss: 0.0744 - acc: 0.9688\n",
      "Epoch 14/100\n",
      "5616/5616 [==============================] - 1s 142us/step - loss: 0.0734 - acc: 0.9688\n",
      "Epoch 15/100\n",
      "5616/5616 [==============================] - 1s 150us/step - loss: 0.0726 - acc: 0.9688\n",
      "Epoch 16/100\n",
      "5616/5616 [==============================] - 1s 155us/step - loss: 0.0720 - acc: 0.9688\n",
      "Epoch 17/100\n",
      "5616/5616 [==============================] - 1s 139us/step - loss: 0.0713 - acc: 0.9688\n",
      "Epoch 18/100\n",
      "5616/5616 [==============================] - 1s 142us/step - loss: 0.0709 - acc: 0.9688\n",
      "Epoch 19/100\n",
      "5616/5616 [==============================] - 1s 141us/step - loss: 0.0703 - acc: 0.9688\n",
      "Epoch 20/100\n",
      "5616/5616 [==============================] - 1s 145us/step - loss: 0.0698 - acc: 0.9688\n",
      "Epoch 21/100\n",
      "5616/5616 [==============================] - 1s 137us/step - loss: 0.0694 - acc: 0.9688\n",
      "Epoch 22/100\n",
      "5616/5616 [==============================] - 1s 144us/step - loss: 0.0689 - acc: 0.9688\n",
      "Epoch 23/100\n",
      "5616/5616 [==============================] - 1s 144us/step - loss: 0.0687 - acc: 0.9688\n",
      "Epoch 24/100\n",
      "5616/5616 [==============================] - 1s 140us/step - loss: 0.0684 - acc: 0.9688\n",
      "Epoch 25/100\n",
      "5616/5616 [==============================] - 1s 141us/step - loss: 0.0680 - acc: 0.9688\n",
      "Epoch 26/100\n",
      "5616/5616 [==============================] - 1s 150us/step - loss: 0.0680 - acc: 0.9688\n",
      "Epoch 27/100\n",
      "5616/5616 [==============================] - 1s 145us/step - loss: 0.0674 - acc: 0.9688\n",
      "Epoch 28/100\n",
      "5616/5616 [==============================] - 1s 141us/step - loss: 0.0670 - acc: 0.9688\n",
      "Epoch 29/100\n",
      "5616/5616 [==============================] - 1s 145us/step - loss: 0.0669 - acc: 0.9688\n",
      "Epoch 30/100\n",
      "5616/5616 [==============================] - 1s 141us/step - loss: 0.0667 - acc: 0.9688\n",
      "Epoch 31/100\n",
      "5616/5616 [==============================] - 1s 145us/step - loss: 0.0659 - acc: 0.9688\n",
      "Epoch 32/100\n",
      "5616/5616 [==============================] - 1s 148us/step - loss: 0.0659 - acc: 0.9688\n",
      "Epoch 33/100\n",
      "5616/5616 [==============================] - 1s 143us/step - loss: 0.0658 - acc: 0.9688\n",
      "Epoch 34/100\n",
      "5616/5616 [==============================] - 1s 145us/step - loss: 0.0657 - acc: 0.9688\n",
      "Epoch 35/100\n",
      "5616/5616 [==============================] - 1s 147us/step - loss: 0.0653 - acc: 0.9688\n",
      "Epoch 36/100\n",
      "5616/5616 [==============================] - 1s 148us/step - loss: 0.0651 - acc: 0.9688\n",
      "Epoch 37/100\n",
      "5616/5616 [==============================] - 1s 163us/step - loss: 0.0647 - acc: 0.9688\n",
      "Epoch 38/100\n",
      "5616/5616 [==============================] - 1s 166us/step - loss: 0.0645 - acc: 0.9688\n",
      "Epoch 39/100\n",
      "5616/5616 [==============================] - 1s 147us/step - loss: 0.0646 - acc: 0.9688\n",
      "Epoch 40/100\n",
      "5616/5616 [==============================] - 1s 145us/step - loss: 0.0641 - acc: 0.9688\n",
      "Epoch 41/100\n",
      "5616/5616 [==============================] - 1s 145us/step - loss: 0.0643 - acc: 0.9676\n",
      "Epoch 42/100\n",
      "5616/5616 [==============================] - 1s 139us/step - loss: 0.0641 - acc: 0.9681\n",
      "Epoch 43/100\n",
      "5616/5616 [==============================] - 1s 141us/step - loss: 0.0640 - acc: 0.9694\n",
      "Epoch 44/100\n",
      "5616/5616 [==============================] - 1s 139us/step - loss: 0.0636 - acc: 0.9701\n",
      "Epoch 45/100\n",
      "5616/5616 [==============================] - 1s 158us/step - loss: 0.0635 - acc: 0.9694\n",
      "Epoch 46/100\n",
      "5616/5616 [==============================] - 1s 149us/step - loss: 0.0633 - acc: 0.9703\n",
      "Epoch 47/100\n",
      "5616/5616 [==============================] - 1s 139us/step - loss: 0.0637 - acc: 0.9706\n",
      "Epoch 48/100\n",
      "5616/5616 [==============================] - 1s 143us/step - loss: 0.0631 - acc: 0.9712\n",
      "Epoch 49/100\n",
      "5616/5616 [==============================] - 1s 148us/step - loss: 0.0635 - acc: 0.9715\n",
      "Epoch 50/100\n",
      "5616/5616 [==============================] - 1s 154us/step - loss: 0.0632 - acc: 0.9710\n",
      "Epoch 51/100\n",
      "5616/5616 [==============================] - 1s 144us/step - loss: 0.0631 - acc: 0.9712\n",
      "Epoch 52/100\n",
      "5616/5616 [==============================] - 1s 148us/step - loss: 0.0629 - acc: 0.9717\n",
      "Epoch 53/100\n",
      "5616/5616 [==============================] - 1s 144us/step - loss: 0.0631 - acc: 0.9703\n",
      "Epoch 54/100\n",
      "5616/5616 [==============================] - 1s 142us/step - loss: 0.0627 - acc: 0.9713\n",
      "Epoch 55/100\n",
      "5616/5616 [==============================] - 1s 144us/step - loss: 0.0628 - acc: 0.9704\n",
      "Epoch 56/100\n",
      "5616/5616 [==============================] - 1s 139us/step - loss: 0.0628 - acc: 0.9713\n",
      "Epoch 57/100\n",
      "5616/5616 [==============================] - 1s 141us/step - loss: 0.0624 - acc: 0.9719\n",
      "Epoch 58/100\n",
      "5616/5616 [==============================] - 1s 149us/step - loss: 0.0622 - acc: 0.9708\n",
      "Epoch 59/100\n",
      "5616/5616 [==============================] - 1s 147us/step - loss: 0.0623 - acc: 0.9712\n",
      "Epoch 60/100\n",
      "5616/5616 [==============================] - 1s 177us/step - loss: 0.0623 - acc: 0.9708\n",
      "Epoch 61/100\n",
      "5616/5616 [==============================] - 1s 150us/step - loss: 0.0625 - acc: 0.9710\n",
      "Epoch 62/100\n",
      "5616/5616 [==============================] - 1s 151us/step - loss: 0.0619 - acc: 0.9712\n",
      "Epoch 63/100\n",
      "5616/5616 [==============================] - 1s 149us/step - loss: 0.0620 - acc: 0.9713\n",
      "Epoch 64/100\n",
      "5616/5616 [==============================] - 1s 150us/step - loss: 0.0618 - acc: 0.9719\n",
      "Epoch 65/100\n",
      "5616/5616 [==============================] - 1s 156us/step - loss: 0.0616 - acc: 0.9724\n",
      "Epoch 66/100\n",
      "5616/5616 [==============================] - 1s 163us/step - loss: 0.0618 - acc: 0.9719\n",
      "Epoch 67/100\n",
      "5616/5616 [==============================] - 1s 147us/step - loss: 0.0620 - acc: 0.9719\n",
      "Epoch 68/100\n",
      "5616/5616 [==============================] - 1s 145us/step - loss: 0.0621 - acc: 0.9708\n",
      "Epoch 69/100\n",
      "5616/5616 [==============================] - 1s 143us/step - loss: 0.0615 - acc: 0.9722\n",
      "Epoch 70/100\n",
      "5616/5616 [==============================] - 1s 148us/step - loss: 0.0617 - acc: 0.9715\n",
      "Epoch 71/100\n",
      "5616/5616 [==============================] - 1s 151us/step - loss: 0.0610 - acc: 0.9720\n",
      "Epoch 72/100\n",
      "5616/5616 [==============================] - 1s 146us/step - loss: 0.0615 - acc: 0.9722\n",
      "Epoch 73/100\n",
      "5616/5616 [==============================] - 1s 142us/step - loss: 0.0611 - acc: 0.9724\n",
      "Epoch 74/100\n",
      "5616/5616 [==============================] - 1s 147us/step - loss: 0.0609 - acc: 0.9731\n",
      "Epoch 75/100\n",
      "5616/5616 [==============================] - 1s 151us/step - loss: 0.0614 - acc: 0.9726\n",
      "Epoch 76/100\n",
      "5616/5616 [==============================] - 1s 159us/step - loss: 0.0610 - acc: 0.9740\n",
      "Epoch 77/100\n",
      "5616/5616 [==============================] - 1s 156us/step - loss: 0.0612 - acc: 0.9731\n",
      "Epoch 78/100\n",
      "5616/5616 [==============================] - 1s 155us/step - loss: 0.0610 - acc: 0.9728\n",
      "Epoch 79/100\n",
      "5616/5616 [==============================] - 1s 146us/step - loss: 0.0607 - acc: 0.9735\n",
      "Epoch 80/100\n",
      "5616/5616 [==============================] - 1s 148us/step - loss: 0.0610 - acc: 0.9720\n",
      "Epoch 81/100\n",
      "5616/5616 [==============================] - 1s 156us/step - loss: 0.0611 - acc: 0.9724\n",
      "Epoch 82/100\n",
      "5616/5616 [==============================] - 1s 148us/step - loss: 0.0601 - acc: 0.9722\n",
      "Epoch 83/100\n",
      "5616/5616 [==============================] - 1s 150us/step - loss: 0.0608 - acc: 0.9724\n",
      "Epoch 84/100\n",
      "5616/5616 [==============================] - 1s 151us/step - loss: 0.0604 - acc: 0.9735\n",
      "Epoch 85/100\n",
      "5616/5616 [==============================] - 1s 157us/step - loss: 0.0607 - acc: 0.9731\n",
      "Epoch 86/100\n",
      "5616/5616 [==============================] - 1s 143us/step - loss: 0.0602 - acc: 0.9720\n",
      "Epoch 87/100\n",
      "5616/5616 [==============================] - 1s 144us/step - loss: 0.0601 - acc: 0.9731\n",
      "Epoch 88/100\n",
      "5616/5616 [==============================] - 1s 141us/step - loss: 0.0610 - acc: 0.9712\n",
      "Epoch 89/100\n",
      "5616/5616 [==============================] - 1s 144us/step - loss: 0.0601 - acc: 0.9724\n",
      "Epoch 90/100\n",
      "5616/5616 [==============================] - 1s 144us/step - loss: 0.0600 - acc: 0.9724\n",
      "Epoch 91/100\n",
      "5616/5616 [==============================] - 1s 147us/step - loss: 0.0600 - acc: 0.9736\n",
      "Epoch 92/100\n",
      "5616/5616 [==============================] - 1s 142us/step - loss: 0.0605 - acc: 0.9728\n",
      "Epoch 93/100\n",
      "5616/5616 [==============================] - 1s 142us/step - loss: 0.0598 - acc: 0.9728\n",
      "Epoch 94/100\n",
      "5616/5616 [==============================] - 1s 144us/step - loss: 0.0601 - acc: 0.9740\n",
      "Epoch 95/100\n",
      "5616/5616 [==============================] - 1s 146us/step - loss: 0.0600 - acc: 0.9736\n",
      "Epoch 96/100\n",
      "5616/5616 [==============================] - 1s 142us/step - loss: 0.0597 - acc: 0.9736\n",
      "Epoch 97/100\n",
      "5616/5616 [==============================] - 1s 146us/step - loss: 0.0597 - acc: 0.9745\n",
      "Epoch 98/100\n",
      "5616/5616 [==============================] - 1s 142us/step - loss: 0.0594 - acc: 0.9735\n",
      "Epoch 99/100\n",
      "5616/5616 [==============================] - 1s 142us/step - loss: 0.0602 - acc: 0.9742\n",
      "Epoch 100/100\n",
      "5616/5616 [==============================] - 1s 146us/step - loss: 0.0591 - acc: 0.9738\n",
      "624/624 [==============================] - 0s 129us/step\n",
      "Epoch 1/100\n",
      "5616/5616 [==============================] - 1s 192us/step - loss: 0.3616 - acc: 0.9653\n",
      "Epoch 2/100\n",
      "5616/5616 [==============================] - 1s 150us/step - loss: 0.1297 - acc: 0.9658\n",
      "Epoch 3/100\n",
      "5616/5616 [==============================] - 1s 151us/step - loss: 0.1036 - acc: 0.9658\n",
      "Epoch 4/100\n",
      "5616/5616 [==============================] - 1s 151us/step - loss: 0.0920 - acc: 0.9658\n",
      "Epoch 5/100\n",
      "5616/5616 [==============================] - 1s 157us/step - loss: 0.0863 - acc: 0.9658\n",
      "Epoch 6/100\n",
      "5616/5616 [==============================] - 1s 148us/step - loss: 0.0833 - acc: 0.9658\n",
      "Epoch 7/100\n",
      "5616/5616 [==============================] - 1s 147us/step - loss: 0.0812 - acc: 0.9658\n",
      "Epoch 8/100\n",
      "5616/5616 [==============================] - 1s 139us/step - loss: 0.0791 - acc: 0.9658\n",
      "Epoch 9/100\n",
      "5616/5616 [==============================] - 1s 153us/step - loss: 0.0780 - acc: 0.9658\n",
      "Epoch 10/100\n",
      "5616/5616 [==============================] - 1s 161us/step - loss: 0.0769 - acc: 0.9658\n",
      "Epoch 11/100\n",
      "5616/5616 [==============================] - 1s 148us/step - loss: 0.0760 - acc: 0.9658\n",
      "Epoch 12/100\n",
      "5616/5616 [==============================] - 1s 150us/step - loss: 0.0750 - acc: 0.9658\n",
      "Epoch 13/100\n",
      "5616/5616 [==============================] - 1s 149us/step - loss: 0.0741 - acc: 0.9658\n",
      "Epoch 14/100\n",
      "5616/5616 [==============================] - 1s 144us/step - loss: 0.0729 - acc: 0.9658\n",
      "Epoch 15/100\n",
      "5616/5616 [==============================] - 1s 150us/step - loss: 0.0725 - acc: 0.9658\n",
      "Epoch 16/100\n",
      "5616/5616 [==============================] - 1s 147us/step - loss: 0.0720 - acc: 0.9658\n",
      "Epoch 17/100\n",
      "5616/5616 [==============================] - 1s 146us/step - loss: 0.0706 - acc: 0.9658\n",
      "Epoch 18/100\n",
      "5616/5616 [==============================] - 1s 150us/step - loss: 0.0701 - acc: 0.9658\n",
      "Epoch 19/100\n",
      "5616/5616 [==============================] - 1s 147us/step - loss: 0.0696 - acc: 0.9658\n",
      "Epoch 20/100\n",
      "5616/5616 [==============================] - 1s 147us/step - loss: 0.0684 - acc: 0.9674\n",
      "Epoch 21/100\n",
      "5616/5616 [==============================] - 1s 144us/step - loss: 0.0681 - acc: 0.9663\n",
      "Epoch 22/100\n",
      "5616/5616 [==============================] - 1s 151us/step - loss: 0.0674 - acc: 0.9701\n",
      "Epoch 23/100\n",
      "5616/5616 [==============================] - 1s 145us/step - loss: 0.0669 - acc: 0.9726\n",
      "Epoch 24/100\n",
      "5616/5616 [==============================] - 1s 142us/step - loss: 0.0663 - acc: 0.9708\n",
      "Epoch 25/100\n",
      "5616/5616 [==============================] - 1s 152us/step - loss: 0.0658 - acc: 0.9722\n",
      "Epoch 26/100\n",
      "5616/5616 [==============================] - 1s 152us/step - loss: 0.0654 - acc: 0.9726\n",
      "Epoch 27/100\n",
      "5616/5616 [==============================] - 1s 148us/step - loss: 0.0643 - acc: 0.9735\n",
      "Epoch 28/100\n",
      "5616/5616 [==============================] - 1s 145us/step - loss: 0.0646 - acc: 0.9717\n",
      "Epoch 29/100\n",
      "5616/5616 [==============================] - 1s 153us/step - loss: 0.0638 - acc: 0.9722\n",
      "Epoch 30/100\n",
      "5616/5616 [==============================] - 1s 145us/step - loss: 0.0635 - acc: 0.9729\n",
      "Epoch 31/100\n",
      "5616/5616 [==============================] - 1s 142us/step - loss: 0.0633 - acc: 0.9747\n",
      "Epoch 32/100\n",
      "5616/5616 [==============================] - 1s 151us/step - loss: 0.0631 - acc: 0.9722\n",
      "Epoch 33/100\n",
      "5616/5616 [==============================] - 1s 151us/step - loss: 0.0623 - acc: 0.9726\n",
      "Epoch 34/100\n",
      "5616/5616 [==============================] - 1s 153us/step - loss: 0.0621 - acc: 0.9726\n",
      "Epoch 35/100\n",
      "5616/5616 [==============================] - 1s 141us/step - loss: 0.0619 - acc: 0.9740\n",
      "Epoch 36/100\n",
      "5616/5616 [==============================] - 1s 126us/step - loss: 0.0621 - acc: 0.9735\n",
      "Epoch 37/100\n",
      "5616/5616 [==============================] - 1s 123us/step - loss: 0.0614 - acc: 0.9736\n",
      "Epoch 38/100\n",
      "5616/5616 [==============================] - 1s 123us/step - loss: 0.0613 - acc: 0.9745\n",
      "Epoch 39/100\n",
      "5616/5616 [==============================] - 1s 131us/step - loss: 0.0609 - acc: 0.9740\n",
      "Epoch 40/100\n",
      "5616/5616 [==============================] - 1s 134us/step - loss: 0.0606 - acc: 0.9749\n",
      "Epoch 41/100\n",
      "5616/5616 [==============================] - 1s 125us/step - loss: 0.0604 - acc: 0.9752\n",
      "Epoch 42/100\n",
      "5616/5616 [==============================] - 1s 126us/step - loss: 0.0601 - acc: 0.9744\n",
      "Epoch 43/100\n",
      "5616/5616 [==============================] - 1s 131us/step - loss: 0.0603 - acc: 0.9752\n",
      "Epoch 44/100\n",
      "5616/5616 [==============================] - 1s 127us/step - loss: 0.0592 - acc: 0.9760\n",
      "Epoch 45/100\n",
      "5616/5616 [==============================] - 1s 137us/step - loss: 0.0598 - acc: 0.9738\n",
      "Epoch 46/100\n",
      "5616/5616 [==============================] - 1s 137us/step - loss: 0.0595 - acc: 0.9745\n",
      "Epoch 47/100\n",
      "5616/5616 [==============================] - 1s 152us/step - loss: 0.0590 - acc: 0.9756\n",
      "Epoch 48/100\n",
      "5616/5616 [==============================] - 1s 155us/step - loss: 0.0585 - acc: 0.9751\n",
      "Epoch 49/100\n",
      "5616/5616 [==============================] - 1s 171us/step - loss: 0.0584 - acc: 0.9763\n",
      "Epoch 50/100\n",
      "5616/5616 [==============================] - 1s 161us/step - loss: 0.0583 - acc: 0.9751\n",
      "Epoch 51/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0585 - acc: 0.9770\n",
      "Epoch 52/100\n",
      "5616/5616 [==============================] - 1s 127us/step - loss: 0.0581 - acc: 0.9774\n",
      "Epoch 53/100\n",
      "5616/5616 [==============================] - 1s 124us/step - loss: 0.0583 - acc: 0.9767\n",
      "Epoch 54/100\n",
      "5616/5616 [==============================] - 1s 122us/step - loss: 0.0576 - acc: 0.9763\n",
      "Epoch 55/100\n",
      "5616/5616 [==============================] - 1s 125us/step - loss: 0.0580 - acc: 0.9758\n",
      "Epoch 56/100\n",
      "5616/5616 [==============================] - 1s 127us/step - loss: 0.0568 - acc: 0.9779\n",
      "Epoch 57/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0578 - acc: 0.9754\n",
      "Epoch 58/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0573 - acc: 0.9769\n",
      "Epoch 59/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0570 - acc: 0.9761\n",
      "Epoch 60/100\n",
      "5616/5616 [==============================] - 1s 137us/step - loss: 0.0571 - acc: 0.9756\n",
      "Epoch 61/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0569 - acc: 0.9760\n",
      "Epoch 62/100\n",
      "5616/5616 [==============================] - 1s 158us/step - loss: 0.0567 - acc: 0.9758\n",
      "Epoch 63/100\n",
      "5616/5616 [==============================] - 1s 160us/step - loss: 0.0565 - acc: 0.9765\n",
      "Epoch 64/100\n",
      "5616/5616 [==============================] - 1s 152us/step - loss: 0.0561 - acc: 0.9779\n",
      "Epoch 65/100\n",
      "5616/5616 [==============================] - 1s 149us/step - loss: 0.0560 - acc: 0.9761\n",
      "Epoch 66/100\n",
      "5616/5616 [==============================] - 1s 123us/step - loss: 0.0552 - acc: 0.9774\n",
      "Epoch 67/100\n",
      "5616/5616 [==============================] - 1s 118us/step - loss: 0.0558 - acc: 0.9770\n",
      "Epoch 68/100\n",
      "5616/5616 [==============================] - 1s 124us/step - loss: 0.0558 - acc: 0.9769\n",
      "Epoch 69/100\n",
      "5616/5616 [==============================] - 1s 126us/step - loss: 0.0557 - acc: 0.9770\n",
      "Epoch 70/100\n",
      "5616/5616 [==============================] - 1s 121us/step - loss: 0.0559 - acc: 0.9761\n",
      "Epoch 71/100\n",
      "5616/5616 [==============================] - 1s 121us/step - loss: 0.0551 - acc: 0.9776\n",
      "Epoch 72/100\n",
      "5616/5616 [==============================] - 1s 124us/step - loss: 0.0546 - acc: 0.9774\n",
      "Epoch 73/100\n",
      "5616/5616 [==============================] - 1s 126us/step - loss: 0.0546 - acc: 0.9781\n",
      "Epoch 74/100\n",
      "5616/5616 [==============================] - 1s 128us/step - loss: 0.0550 - acc: 0.9774\n",
      "Epoch 75/100\n",
      "5616/5616 [==============================] - 1s 116us/step - loss: 0.0551 - acc: 0.9756\n",
      "Epoch 76/100\n",
      "5616/5616 [==============================] - 1s 122us/step - loss: 0.0540 - acc: 0.9779\n",
      "Epoch 77/100\n",
      "5616/5616 [==============================] - 1s 127us/step - loss: 0.0548 - acc: 0.9758\n",
      "Epoch 78/100\n",
      "5616/5616 [==============================] - 1s 168us/step - loss: 0.0543 - acc: 0.9772\n",
      "Epoch 79/100\n",
      "5616/5616 [==============================] - 1s 134us/step - loss: 0.0540 - acc: 0.9776\n",
      "Epoch 80/100\n",
      "5616/5616 [==============================] - 1s 128us/step - loss: 0.0548 - acc: 0.9761\n",
      "Epoch 81/100\n",
      "5616/5616 [==============================] - 1s 119us/step - loss: 0.0540 - acc: 0.9765\n",
      "Epoch 82/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0539 - acc: 0.9770\n",
      "Epoch 83/100\n",
      "5616/5616 [==============================] - 1s 125us/step - loss: 0.0542 - acc: 0.9763\n",
      "Epoch 84/100\n",
      "5616/5616 [==============================] - 1s 126us/step - loss: 0.0539 - acc: 0.9767\n",
      "Epoch 85/100\n",
      "5616/5616 [==============================] - 1s 126us/step - loss: 0.0537 - acc: 0.9770\n",
      "Epoch 86/100\n",
      "5616/5616 [==============================] - 1s 124us/step - loss: 0.0534 - acc: 0.9770\n",
      "Epoch 87/100\n",
      "5616/5616 [==============================] - 1s 124us/step - loss: 0.0535 - acc: 0.9765\n",
      "Epoch 88/100\n",
      "5616/5616 [==============================] - 1s 122us/step - loss: 0.0540 - acc: 0.9763\n",
      "Epoch 89/100\n",
      "5616/5616 [==============================] - 1s 129us/step - loss: 0.0533 - acc: 0.9769\n",
      "Epoch 90/100\n",
      "5616/5616 [==============================] - 1s 122us/step - loss: 0.0531 - acc: 0.9783\n",
      "Epoch 91/100\n",
      "5616/5616 [==============================] - 1s 126us/step - loss: 0.0530 - acc: 0.9772\n",
      "Epoch 92/100\n",
      "5616/5616 [==============================] - 1s 121us/step - loss: 0.0531 - acc: 0.9770\n",
      "Epoch 93/100\n",
      "5616/5616 [==============================] - 1s 119us/step - loss: 0.0534 - acc: 0.9793\n",
      "Epoch 94/100\n",
      "5616/5616 [==============================] - 1s 124us/step - loss: 0.0532 - acc: 0.9779\n",
      "Epoch 95/100\n",
      "5616/5616 [==============================] - 1s 123us/step - loss: 0.0531 - acc: 0.9779\n",
      "Epoch 96/100\n",
      "5616/5616 [==============================] - 1s 128us/step - loss: 0.0533 - acc: 0.9779\n",
      "Epoch 97/100\n",
      "5616/5616 [==============================] - 1s 126us/step - loss: 0.0531 - acc: 0.9769\n",
      "Epoch 98/100\n",
      "5616/5616 [==============================] - 1s 119us/step - loss: 0.0522 - acc: 0.9792\n",
      "Epoch 99/100\n",
      "5616/5616 [==============================] - 1s 120us/step - loss: 0.0522 - acc: 0.9777\n",
      "Epoch 100/100\n",
      "5616/5616 [==============================] - 1s 128us/step - loss: 0.0530 - acc: 0.9777\n",
      "624/624 [==============================] - 0s 121us/step\n",
      "Epoch 1/100\n",
      "5616/5616 [==============================] - 1s 165us/step - loss: 0.3512 - acc: 0.9672\n",
      "Epoch 2/100\n",
      "5616/5616 [==============================] - 1s 128us/step - loss: 0.1266 - acc: 0.9674\n",
      "Epoch 3/100\n",
      "5616/5616 [==============================] - 1s 128us/step - loss: 0.1028 - acc: 0.9674\n",
      "Epoch 4/100\n",
      "5616/5616 [==============================] - 1s 119us/step - loss: 0.0922 - acc: 0.9674\n",
      "Epoch 5/100\n",
      "5616/5616 [==============================] - 1s 122us/step - loss: 0.0865 - acc: 0.9674\n",
      "Epoch 6/100\n",
      "5616/5616 [==============================] - 1s 118us/step - loss: 0.0833 - acc: 0.9674\n",
      "Epoch 7/100\n",
      "5616/5616 [==============================] - 1s 127us/step - loss: 0.0813 - acc: 0.9674\n",
      "Epoch 8/100\n",
      "5616/5616 [==============================] - 1s 124us/step - loss: 0.0796 - acc: 0.9674\n",
      "Epoch 9/100\n",
      "5616/5616 [==============================] - 1s 121us/step - loss: 0.0776 - acc: 0.9674\n",
      "Epoch 10/100\n",
      "5616/5616 [==============================] - 1s 124us/step - loss: 0.0772 - acc: 0.9674\n",
      "Epoch 11/100\n",
      "5616/5616 [==============================] - 1s 124us/step - loss: 0.0757 - acc: 0.9674\n",
      "Epoch 12/100\n",
      "5616/5616 [==============================] - 1s 124us/step - loss: 0.0753 - acc: 0.9674\n",
      "Epoch 13/100\n",
      "5616/5616 [==============================] - 1s 126us/step - loss: 0.0741 - acc: 0.9674\n",
      "Epoch 14/100\n",
      "5616/5616 [==============================] - 1s 124us/step - loss: 0.0732 - acc: 0.9674\n",
      "Epoch 15/100\n",
      "5616/5616 [==============================] - 1s 122us/step - loss: 0.0723 - acc: 0.9674\n",
      "Epoch 16/100\n",
      "5616/5616 [==============================] - 1s 129us/step - loss: 0.0724 - acc: 0.9674\n",
      "Epoch 17/100\n",
      "5616/5616 [==============================] - 1s 126us/step - loss: 0.0711 - acc: 0.9674\n",
      "Epoch 18/100\n",
      "5616/5616 [==============================] - 1s 128us/step - loss: 0.0709 - acc: 0.9674\n",
      "Epoch 19/100\n",
      "5616/5616 [==============================] - 1s 127us/step - loss: 0.0706 - acc: 0.9674\n",
      "Epoch 20/100\n",
      "5616/5616 [==============================] - 1s 124us/step - loss: 0.0700 - acc: 0.9674\n",
      "Epoch 21/100\n",
      "5616/5616 [==============================] - 1s 124us/step - loss: 0.0700 - acc: 0.9674\n",
      "Epoch 22/100\n",
      "5616/5616 [==============================] - 1s 123us/step - loss: 0.0693 - acc: 0.9674\n",
      "Epoch 23/100\n",
      "5616/5616 [==============================] - 1s 127us/step - loss: 0.0690 - acc: 0.9674\n",
      "Epoch 24/100\n",
      "5616/5616 [==============================] - 1s 135us/step - loss: 0.0683 - acc: 0.9674\n",
      "Epoch 25/100\n",
      "5616/5616 [==============================] - 1s 123us/step - loss: 0.0677 - acc: 0.9674\n",
      "Epoch 26/100\n",
      "5616/5616 [==============================] - 1s 123us/step - loss: 0.0673 - acc: 0.9674\n",
      "Epoch 27/100\n",
      "5616/5616 [==============================] - 1s 122us/step - loss: 0.0675 - acc: 0.9674\n",
      "Epoch 28/100\n",
      "5616/5616 [==============================] - 1s 125us/step - loss: 0.0666 - acc: 0.9674\n",
      "Epoch 29/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0666 - acc: 0.9674\n",
      "Epoch 30/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0661 - acc: 0.9674\n",
      "Epoch 31/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0657 - acc: 0.9674\n",
      "Epoch 32/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0651 - acc: 0.9674\n",
      "Epoch 33/100\n",
      "5616/5616 [==============================] - 1s 144us/step - loss: 0.0647 - acc: 0.9679\n",
      "Epoch 34/100\n",
      "5616/5616 [==============================] - 1s 130us/step - loss: 0.0648 - acc: 0.9704\n",
      "Epoch 35/100\n",
      "5616/5616 [==============================] - 1s 129us/step - loss: 0.0638 - acc: 0.9706\n",
      "Epoch 36/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0638 - acc: 0.9701\n",
      "Epoch 37/100\n",
      "5616/5616 [==============================] - 1s 128us/step - loss: 0.0632 - acc: 0.9715\n",
      "Epoch 38/100\n",
      "5616/5616 [==============================] - 1s 147us/step - loss: 0.0633 - acc: 0.9724\n",
      "Epoch 39/100\n",
      "5616/5616 [==============================] - 1s 135us/step - loss: 0.0624 - acc: 0.9719\n",
      "Epoch 40/100\n",
      "5616/5616 [==============================] - 1s 138us/step - loss: 0.0625 - acc: 0.9712\n",
      "Epoch 41/100\n",
      "5616/5616 [==============================] - 1s 129us/step - loss: 0.0616 - acc: 0.9728\n",
      "Epoch 42/100\n",
      "5616/5616 [==============================] - 1s 135us/step - loss: 0.0619 - acc: 0.9726\n",
      "Epoch 43/100\n",
      "5616/5616 [==============================] - 1s 135us/step - loss: 0.0611 - acc: 0.9726\n",
      "Epoch 44/100\n",
      "5616/5616 [==============================] - 1s 135us/step - loss: 0.0612 - acc: 0.9720\n",
      "Epoch 45/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0609 - acc: 0.9724\n",
      "Epoch 46/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0606 - acc: 0.9735\n",
      "Epoch 47/100\n",
      "5616/5616 [==============================] - 1s 134us/step - loss: 0.0600 - acc: 0.9722\n",
      "Epoch 48/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0601 - acc: 0.9733\n",
      "Epoch 49/100\n",
      "5616/5616 [==============================] - 1s 130us/step - loss: 0.0600 - acc: 0.9736\n",
      "Epoch 50/100\n",
      "5616/5616 [==============================] - 1s 127us/step - loss: 0.0593 - acc: 0.9731\n",
      "Epoch 51/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0588 - acc: 0.9744\n",
      "Epoch 52/100\n",
      "5616/5616 [==============================] - 1s 131us/step - loss: 0.0592 - acc: 0.9740\n",
      "Epoch 53/100\n",
      "5616/5616 [==============================] - 1s 134us/step - loss: 0.0589 - acc: 0.9735\n",
      "Epoch 54/100\n",
      "5616/5616 [==============================] - 1s 134us/step - loss: 0.0585 - acc: 0.9738\n",
      "Epoch 55/100\n",
      "5616/5616 [==============================] - 1s 141us/step - loss: 0.0584 - acc: 0.9736\n",
      "Epoch 56/100\n",
      "5616/5616 [==============================] - 1s 137us/step - loss: 0.0579 - acc: 0.9749\n",
      "Epoch 57/100\n",
      "5616/5616 [==============================] - 1s 141us/step - loss: 0.0578 - acc: 0.9745\n",
      "Epoch 58/100\n",
      "5616/5616 [==============================] - 1s 129us/step - loss: 0.0582 - acc: 0.9752\n",
      "Epoch 59/100\n",
      "5616/5616 [==============================] - 1s 128us/step - loss: 0.0575 - acc: 0.9756\n",
      "Epoch 60/100\n",
      "5616/5616 [==============================] - 1s 127us/step - loss: 0.0576 - acc: 0.9752\n",
      "Epoch 61/100\n",
      "5616/5616 [==============================] - 1s 127us/step - loss: 0.0571 - acc: 0.9745\n",
      "Epoch 62/100\n",
      "5616/5616 [==============================] - 1s 134us/step - loss: 0.0571 - acc: 0.9756\n",
      "Epoch 63/100\n",
      "5616/5616 [==============================] - 1s 134us/step - loss: 0.0574 - acc: 0.9754\n",
      "Epoch 64/100\n",
      "5616/5616 [==============================] - 1s 136us/step - loss: 0.0570 - acc: 0.9758\n",
      "Epoch 65/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0565 - acc: 0.9756\n",
      "Epoch 66/100\n",
      "5616/5616 [==============================] - 1s 130us/step - loss: 0.0565 - acc: 0.9751\n",
      "Epoch 67/100\n",
      "5616/5616 [==============================] - 1s 136us/step - loss: 0.0566 - acc: 0.9756\n",
      "Epoch 68/100\n",
      "5616/5616 [==============================] - 1s 144us/step - loss: 0.0563 - acc: 0.9761\n",
      "Epoch 69/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0564 - acc: 0.9765\n",
      "Epoch 70/100\n",
      "5616/5616 [==============================] - 1s 130us/step - loss: 0.0563 - acc: 0.9756\n",
      "Epoch 71/100\n",
      "5616/5616 [==============================] - 1s 126us/step - loss: 0.0565 - acc: 0.9774\n",
      "Epoch 72/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0557 - acc: 0.9769\n",
      "Epoch 73/100\n",
      "5616/5616 [==============================] - 1s 139us/step - loss: 0.0565 - acc: 0.9763\n",
      "Epoch 74/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0560 - acc: 0.9760\n",
      "Epoch 75/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0557 - acc: 0.9760\n",
      "Epoch 76/100\n",
      "5616/5616 [==============================] - 1s 139us/step - loss: 0.0561 - acc: 0.9756\n",
      "Epoch 77/100\n",
      "5616/5616 [==============================] - 1s 136us/step - loss: 0.0555 - acc: 0.9763\n",
      "Epoch 78/100\n",
      "5616/5616 [==============================] - 1s 134us/step - loss: 0.0562 - acc: 0.9769\n",
      "Epoch 79/100\n",
      "5616/5616 [==============================] - 1s 135us/step - loss: 0.0551 - acc: 0.9772\n",
      "Epoch 80/100\n",
      "5616/5616 [==============================] - 1s 144us/step - loss: 0.0558 - acc: 0.9767\n",
      "Epoch 81/100\n",
      "5616/5616 [==============================] - 1s 138us/step - loss: 0.0554 - acc: 0.9751\n",
      "Epoch 82/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0555 - acc: 0.9761\n",
      "Epoch 83/100\n",
      "5616/5616 [==============================] - 1s 129us/step - loss: 0.0553 - acc: 0.9754\n",
      "Epoch 84/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0549 - acc: 0.9749\n",
      "Epoch 85/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0550 - acc: 0.9774\n",
      "Epoch 86/100\n",
      "5616/5616 [==============================] - 1s 129us/step - loss: 0.0550 - acc: 0.9769\n",
      "Epoch 87/100\n",
      "5616/5616 [==============================] - 1s 125us/step - loss: 0.0556 - acc: 0.9763\n",
      "Epoch 88/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0547 - acc: 0.9767\n",
      "Epoch 89/100\n",
      "5616/5616 [==============================] - 1s 131us/step - loss: 0.0546 - acc: 0.9760\n",
      "Epoch 90/100\n",
      "5616/5616 [==============================] - 1s 129us/step - loss: 0.0544 - acc: 0.9760\n",
      "Epoch 91/100\n",
      "5616/5616 [==============================] - 1s 129us/step - loss: 0.0544 - acc: 0.9769\n",
      "Epoch 92/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0545 - acc: 0.9786\n",
      "Epoch 93/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0545 - acc: 0.9767\n",
      "Epoch 94/100\n",
      "5616/5616 [==============================] - 1s 131us/step - loss: 0.0544 - acc: 0.9763\n",
      "Epoch 95/100\n",
      "5616/5616 [==============================] - 1s 130us/step - loss: 0.0539 - acc: 0.9785\n",
      "Epoch 96/100\n",
      "5616/5616 [==============================] - 1s 134us/step - loss: 0.0542 - acc: 0.9774\n",
      "Epoch 97/100\n",
      "5616/5616 [==============================] - 1s 130us/step - loss: 0.0539 - acc: 0.9779\n",
      "Epoch 98/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0541 - acc: 0.9763\n",
      "Epoch 99/100\n",
      "5616/5616 [==============================] - 1s 126us/step - loss: 0.0539 - acc: 0.9779\n",
      "Epoch 100/100\n",
      "5616/5616 [==============================] - 1s 135us/step - loss: 0.0538 - acc: 0.9772\n",
      "624/624 [==============================] - 0s 157us/step\n",
      "Epoch 1/100\n",
      "5616/5616 [==============================] - 1s 180us/step - loss: 0.4477 - acc: 0.9667\n",
      "Epoch 2/100\n",
      "5616/5616 [==============================] - 1s 130us/step - loss: 0.1289 - acc: 0.9678\n",
      "Epoch 3/100\n",
      "5616/5616 [==============================] - 1s 131us/step - loss: 0.0905 - acc: 0.9678\n",
      "Epoch 4/100\n",
      "5616/5616 [==============================] - 1s 139us/step - loss: 0.0814 - acc: 0.9678\n",
      "Epoch 5/100\n",
      "5616/5616 [==============================] - 1s 130us/step - loss: 0.0771 - acc: 0.9678\n",
      "Epoch 6/100\n",
      "5616/5616 [==============================] - 1s 129us/step - loss: 0.0745 - acc: 0.9678\n",
      "Epoch 7/100\n",
      "5616/5616 [==============================] - 1s 131us/step - loss: 0.0726 - acc: 0.9678\n",
      "Epoch 8/100\n",
      "5616/5616 [==============================] - 1s 134us/step - loss: 0.0712 - acc: 0.9678\n",
      "Epoch 9/100\n",
      "5616/5616 [==============================] - 1s 130us/step - loss: 0.0695 - acc: 0.9678\n",
      "Epoch 10/100\n",
      "5616/5616 [==============================] - 1s 134us/step - loss: 0.0682 - acc: 0.9678\n",
      "Epoch 11/100\n",
      "5616/5616 [==============================] - 1s 138us/step - loss: 0.0671 - acc: 0.9678\n",
      "Epoch 12/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0663 - acc: 0.9678\n",
      "Epoch 13/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0650 - acc: 0.9678\n",
      "Epoch 14/100\n",
      "5616/5616 [==============================] - 1s 129us/step - loss: 0.0645 - acc: 0.9678\n",
      "Epoch 15/100\n",
      "5616/5616 [==============================] - 1s 136us/step - loss: 0.0639 - acc: 0.9678\n",
      "Epoch 16/100\n",
      "5616/5616 [==============================] - 1s 129us/step - loss: 0.0632 - acc: 0.9678\n",
      "Epoch 17/100\n",
      "5616/5616 [==============================] - 1s 130us/step - loss: 0.0629 - acc: 0.9722\n",
      "Epoch 18/100\n",
      "5616/5616 [==============================] - 1s 131us/step - loss: 0.0622 - acc: 0.9731\n",
      "Epoch 19/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0619 - acc: 0.9728\n",
      "Epoch 20/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0615 - acc: 0.9738\n",
      "Epoch 21/100\n",
      "5616/5616 [==============================] - 1s 131us/step - loss: 0.0615 - acc: 0.9742\n",
      "Epoch 22/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0601 - acc: 0.9749\n",
      "Epoch 23/100\n",
      "5616/5616 [==============================] - 1s 136us/step - loss: 0.0610 - acc: 0.9735\n",
      "Epoch 24/100\n",
      "5616/5616 [==============================] - 1s 128us/step - loss: 0.0606 - acc: 0.9742\n",
      "Epoch 25/100\n",
      "5616/5616 [==============================] - 1s 131us/step - loss: 0.0603 - acc: 0.9736\n",
      "Epoch 26/100\n",
      "5616/5616 [==============================] - 1s 130us/step - loss: 0.0598 - acc: 0.9736\n",
      "Epoch 27/100\n",
      "5616/5616 [==============================] - 1s 138us/step - loss: 0.0595 - acc: 0.9745\n",
      "Epoch 28/100\n",
      "5616/5616 [==============================] - 1s 127us/step - loss: 0.0593 - acc: 0.9749\n",
      "Epoch 29/100\n",
      "5616/5616 [==============================] - 1s 131us/step - loss: 0.0593 - acc: 0.9744\n",
      "Epoch 30/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0590 - acc: 0.9749\n",
      "Epoch 31/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0587 - acc: 0.9760\n",
      "Epoch 32/100\n",
      "5616/5616 [==============================] - 1s 131us/step - loss: 0.0587 - acc: 0.9761\n",
      "Epoch 33/100\n",
      "5616/5616 [==============================] - 1s 129us/step - loss: 0.0588 - acc: 0.9756\n",
      "Epoch 34/100\n",
      "5616/5616 [==============================] - 1s 128us/step - loss: 0.0584 - acc: 0.9763\n",
      "Epoch 35/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0578 - acc: 0.9761\n",
      "Epoch 36/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0581 - acc: 0.9767\n",
      "Epoch 37/100\n",
      "5616/5616 [==============================] - 1s 131us/step - loss: 0.0579 - acc: 0.9767\n",
      "Epoch 38/100\n",
      "5616/5616 [==============================] - 1s 130us/step - loss: 0.0576 - acc: 0.9767\n",
      "Epoch 39/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0572 - acc: 0.9765\n",
      "Epoch 40/100\n",
      "5616/5616 [==============================] - 1s 139us/step - loss: 0.0572 - acc: 0.9772\n",
      "Epoch 41/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0569 - acc: 0.9754\n",
      "Epoch 42/100\n",
      "5616/5616 [==============================] - 1s 137us/step - loss: 0.0567 - acc: 0.9765\n",
      "Epoch 43/100\n",
      "5616/5616 [==============================] - 1s 138us/step - loss: 0.0566 - acc: 0.9774\n",
      "Epoch 44/100\n",
      "5616/5616 [==============================] - 1s 137us/step - loss: 0.0565 - acc: 0.9767\n",
      "Epoch 45/100\n",
      "5616/5616 [==============================] - 1s 146us/step - loss: 0.0563 - acc: 0.9769\n",
      "Epoch 46/100\n",
      "5616/5616 [==============================] - 1s 140us/step - loss: 0.0561 - acc: 0.9765\n",
      "Epoch 47/100\n",
      "5616/5616 [==============================] - 1s 141us/step - loss: 0.0560 - acc: 0.9763\n",
      "Epoch 48/100\n",
      "5616/5616 [==============================] - 1s 137us/step - loss: 0.0553 - acc: 0.9783\n",
      "Epoch 49/100\n",
      "5616/5616 [==============================] - 1s 139us/step - loss: 0.0555 - acc: 0.9774\n",
      "Epoch 50/100\n",
      "5616/5616 [==============================] - 1s 145us/step - loss: 0.0558 - acc: 0.9777\n",
      "Epoch 51/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0551 - acc: 0.9772\n",
      "Epoch 52/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0556 - acc: 0.9786\n",
      "Epoch 53/100\n",
      "5616/5616 [==============================] - 1s 134us/step - loss: 0.0549 - acc: 0.9792\n",
      "Epoch 54/100\n",
      "5616/5616 [==============================] - 1s 133us/step - loss: 0.0549 - acc: 0.9795\n",
      "Epoch 55/100\n",
      "5616/5616 [==============================] - 1s 139us/step - loss: 0.0559 - acc: 0.9763\n",
      "Epoch 56/100\n",
      "5616/5616 [==============================] - 1s 134us/step - loss: 0.0549 - acc: 0.9776\n",
      "Epoch 57/100\n",
      "5616/5616 [==============================] - 1s 129us/step - loss: 0.0545 - acc: 0.9777\n",
      "Epoch 58/100\n",
      "5616/5616 [==============================] - 1s 131us/step - loss: 0.0551 - acc: 0.9777\n",
      "Epoch 59/100\n",
      "5616/5616 [==============================] - 1s 134us/step - loss: 0.0546 - acc: 0.9786\n",
      "Epoch 60/100\n",
      "5616/5616 [==============================] - 1s 136us/step - loss: 0.0544 - acc: 0.9779\n",
      "Epoch 61/100\n",
      "5616/5616 [==============================] - 1s 139us/step - loss: 0.0547 - acc: 0.9769\n",
      "Epoch 62/100\n",
      "5616/5616 [==============================] - 1s 135us/step - loss: 0.0538 - acc: 0.9779\n",
      "Epoch 63/100\n",
      "5616/5616 [==============================] - 1s 132us/step - loss: 0.0548 - acc: 0.9783\n",
      "Epoch 64/100\n",
      "5616/5616 [==============================] - 1s 137us/step - loss: 0.0540 - acc: 0.9777\n",
      "Epoch 65/100\n",
      "5616/5616 [==============================] - 1s 134us/step - loss: 0.0541 - acc: 0.9783\n",
      "Epoch 66/100\n",
      "5616/5616 [==============================] - 1s 130us/step - loss: 0.0536 - acc: 0.9777\n",
      "Epoch 67/100\n",
      "5616/5616 [==============================] - 1s 134us/step - loss: 0.0543 - acc: 0.9786\n",
      "Epoch 68/100\n",
      "5616/5616 [==============================] - 1s 157us/step - loss: 0.0542 - acc: 0.9783\n",
      "Epoch 69/100\n",
      "5616/5616 [==============================] - 1s 176us/step - loss: 0.0540 - acc: 0.9779\n",
      "Epoch 70/100\n",
      "5616/5616 [==============================] - 1s 163us/step - loss: 0.0532 - acc: 0.9772\n",
      "Epoch 71/100\n",
      "4140/5616 [=====================>........] - ETA: 0s - loss: 0.0514 - acc: 0.9797"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-fdb2c4dcfb98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m                              \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                              \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                              cv=10)\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mvariance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    403\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 240\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample_weight'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2715\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2717\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2718\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2719\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2675\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2676\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2677\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2678\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpuenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluating the ANN\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units=32, # 6 hidden layer nodes (input + output)/2 (11+1)/2\n",
    "                         kernel_initializer='uniform',\n",
    "                         activation='relu', # relu = rectifier function\n",
    "                         input_dim=62)) # 11 features from X\n",
    "    classifier.add(Dense(units=32, # 6 hidden layer nodes\n",
    "                         kernel_initializer='uniform',\n",
    "                         activation='relu'))\n",
    "    classifier.add(Dense(units=1, # 6 hidden layer nodes\n",
    "                         kernel_initializer='uniform',\n",
    "                         activation='sigmoid'))\n",
    "    classifier.compile(optimizer='adam', # gradient descent algorithm (adam algorithm))\n",
    "                       loss='binary_crossentropy',# logarithmic loss algorithm \n",
    "                       metrics=['accuracy']) \n",
    "    return classifier\n",
    "\n",
    "classifier = KerasClassifier(build_fn=build_classifier,\n",
    "                             batch_size=20,\n",
    "                             epochs=100)\n",
    "accuracies = cross_val_score(estimator=classifier,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=10)\n",
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOU\n",
      "[[0.03062681]]\n",
      "\n",
      "NOP\n",
      "[[0.03062681]]\n",
      "\n",
      "GSW\n",
      "[[0.03062681]]\n",
      "\n",
      "TOR\n",
      "[[0.03062681]]\n",
      "\n",
      "LAL\n",
      "[[0.03062681]]\n",
      "\n",
      "PHI\n",
      "[[0.03062681]]\n",
      "\n",
      "OKC\n",
      "[[0.03062681]]\n",
      "\n",
      "MIL\n",
      "[[0.03062681]]\n",
      "\n",
      "POR\n",
      "[[0.03062681]]\n",
      "\n",
      "DET\n",
      "[[0.03062681]]\n",
      "\n",
      "CHA\n",
      "[[0.03062681]]\n",
      "\n",
      "PHX\n",
      "[[0.03062681]]\n",
      "\n",
      "WAS\n",
      "[[0.03062681]]\n",
      "\n",
      "CHI\n",
      "[[0.03062681]]\n",
      "\n",
      "BOS\n",
      "[[0.03062681]]\n",
      "\n",
      "SAS\n",
      "[[0.03062681]]\n",
      "\n",
      "MIN\n",
      "[[0.03062681]]\n",
      "\n",
      "LAC\n",
      "[[0.03062681]]\n",
      "\n",
      "UTA\n",
      "[[0.03062681]]\n",
      "\n",
      "ORL\n",
      "[[0.03062681]]\n",
      "\n",
      "MEM\n",
      "[[0.03062681]]\n",
      "\n",
      "SAC\n",
      "[[0.03062681]]\n",
      "\n",
      "NYK\n",
      "[[0.03062681]]\n",
      "\n",
      "IND\n",
      "[[0.03062681]]\n",
      "\n",
      "DAL\n",
      "[[0.03062681]]\n",
      "\n",
      "DEN\n",
      "[[0.03062681]]\n",
      "\n",
      "CLE\n",
      "[[0.03062681]]\n",
      "\n",
      "ATL\n",
      "[[0.03062681]]\n",
      "\n",
      "BKN\n",
      "[[0.03062681]]\n",
      "\n",
      "MIA\n",
      "[[0.03062681]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_new = pd.read_csv('data/avg/2019-01-11_avg.csv')\n",
    "data_new.loc[data_new.WLP < 1, 'WLP'] = data_new.loc[data_new.WLP < 1, 'WLP'] *100\n",
    "\n",
    "for item in data_new.TEAM.unique():\n",
    "    df_new = data_new[data_new.TEAM == item]\n",
    "    df_new = pd.DataFrame(df_new.groupby('TEAM').mean()).reset_index()\n",
    "    df_new = df_new.drop(['TEAM', '+/-'], axis=1)\n",
    "    print(item)\n",
    "    print(classifier.predict(df_new))\n",
    "    print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
